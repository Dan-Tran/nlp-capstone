---
layout: post
title:  "Blog 9: Continuing Second Advanced Solution Attempt"
---

## What We Tried

This week, we continued improving the model we introduced previously through more preprocessing and more optimizing.  For the preprocessing, instead of having the object detection run each time we load in the image, we ran the object detection separately before we trained and saved the results on disk so that the model can simply look up the list of object for each image instead of recomputing it each time.  Additionally, the images were preprocessed using ResNet. We also preprocessed all of the parsing for every caption so we don't have to run the universal dependency model every time we want to load in the vocabulary. All of this was done to improve training time so that we can actually run for a substantive amount of epochs and more easily tune the model.

Additionally, with the improvements in the training time, we were able to perform optimizations and tuning to our model.  We tried out many different configurations of parameters to see which one worked the best.  These parameters ranged from the shape of the feedforward network, the hidden sizes of our recurrent modules, type of activations, dropout rate, and other miscellaneous parameters. These attempts are further explained in the Experiments section below.

Ultimately, the model that performed the best so far is the following. For each training instance, the two images are encoded using a ResNet, resulting in a 512 output vector for each. The two images are also fed into Yolo, an object detector, outputting a list of objects for each image, the object being represented by its type, probability, and bounding box coordinates. This list of objects is fed into an 100-dimensional bidirectional LSTM to get its encoded object vector. On the caption-side, the text is embedded using 100-dimensional GloVe embeddings. The caption is also run through aa universal dependency parser to get the predicted tags, heads, and dependencies. All three are then embedded. The heads are embedded with the same 100-dimensional GloVe embeddings as the text. The tags are embedded with a basic 50-dimensional embedder and the dependencies are embedded with a different basic 50-dimensional embedder. These embeddings are then concatenated together, with each word vector concatenated with its associated tag, head, and dependency vector. This concatenated embedding is then fed into a bidirectional LSTM with a hidden size of 500 and 2 layers, resulting in an encoded vector for the text.  This vector is then concatenated with the image encodings and object encodings and then fed into a 4 layer feedforward network.

The code for this attempt is located at [https://github.com/Dan-Tran/nlp-capstone/tree/master/blogcode/blog9](https://github.com/Dan-Tran/nlp-capstone/tree/master/blogcode/blog9), with `testmodel.py`, `plaintext_reader.py`, and `test.jsonnet` being the code used for this attempt.

## Experiments

The tuning of the hyperparameters was geared towards the feedforward network in attempts to help each of the modules integrate together. Specifically, we varied the number of layers, the size of the hidden layers, the activation at each layer, and the dropout at each layer.

Below is a table of some of the models we tried. Unless specified otherwise the feedforward layer has all activation functions at each layer being linear and all dropout at each layer being 0 

| Model | Best/Total Epoch | Training Accuracy | Development Accuracy |
| ----- | -----------------| ----------------- | -------------------- |
| 1 layer FF, hidden [2] | 1/4 | 52.7 | 51.0 |
| 1 layer FF, hidden [2] | 6/10 | 53.8 | 51.8 |
| 3 layer FF, hidden [2, 2, 2] | 6/10 | 53.6 | 52.53 |
| 5 layer FF, hidden [2, 2, 2, 2, 2] | 1/5 | 52.5 | 52.2 |
| 3 layer FF, hidden [8, 4, 2], activation [relu, relu, linear] | 0/6 | 50.8 | 50.9 |
| 3 layer FF, hidden [8, 4, 2] | 11/16 | 54.0 | 52.0 |
| **4 layer FF, hidden [16, 8, 4, 2]** | **6/12** | **53.7** | **52.54** |
| 5 layer FF, hidden [32, 16, 8, 4, 2] | 5/14 | 54.1 | 52.48 |
| 6 layer FF, hidden [64, 32, 16, 8, 4, 2] | 8/17 | 53.9 | 52.3 |
| 7 layer FF, hidden [128, 64, 32, 16, 8, 4, 2], activation [sigmoid, relu, relu, relu, relu, relu, relu], dropout [.2, .2, .2, .2, .2, .1, 0] | 0/21 | 50.8 | 50.9 |

**Bolded is the model with the best performance. Other details on the model listed in the previous section.**

The initial experiment of the model using object detection resulted in a training accuracy of 52.7% on training and 51% on development. While the results were very similar to that of random guessing, being able to collect results in a reasonable amount of time, due to preprocessing, opened the possibility of testing hyperparameters (4-hour vocab loading -> 1-2 hour, 25 min/epoch -> 5 min/epoch).

The best model so far is a four-layer feedforward with hidden dimensions of [16, 8, 4, 2] that got 53.7% training accuracy and 52.54% on the development. 

While this increase may initially appear minuscule, it is important to reiterate that the state of the art for this task is ~54% and that random guessing is ~51%.


## Error Analysis

We present error analysis [here](https://docs.google.com/spreadsheets/d/17ShV29iQvFxUC5nOFDXg-vs0AO9mK32oCQubQz85jyk/edit?usp=sharing) on the dev and train set, and our findings follow. The first 3 sentences are from the train set, and the last is from the dev set. First, we notice that for one image, we tend to predict different labels, which is a correct and accurate representation of the data. Next, we examine particular cases that we fail on. One sentence that we classified incorrectly (i.e. we did not do better than random guessing) is `In one image, a single slender spray bottle stands to the left of a box with a woman's face on it.` This is probably because the object detection is unable to detect a helpful object in the image, so it is not able to use object detection to understand the image pair accurately. Our object detection results show one object detected, but the object isn't helpful in identifying a `woman's face on a spray bottle`. Another thing that we noticed is that our detection is rarely consistent; i.e. we rarely get all image pairs associated with a sentence correctly. For example, we commonly get 1/3, 2/4, or 3/4 images pairs correct in association with each sentence. This implies that we need more training epochs to fit the data better, as the model needs to be able to better fit to the images given. When looking at the dev set, we also notice that the labels seem close to random. For example, in the sentence `One image depicts at least a dozen baboons posed on a dry surface.`, we get two labels correct and 2 labels incorrect. This is the same as guessing 50/50 - which implies that we aren't able to generalize to the dev set for this case. This is reinforced by looking close at the train set - there are not any sentences close to `One image depicts at least a dozen baboons posed on a dry surface.`. In general, going forward, we need to likely train for more epochs, and possibly think of ways to generalize better to the development data.

## Next Steps

We plan on continue to tune our model, experimenting on a variety of parameter options like the number of epochs. These parameters will be changed by our observations of the error analysis. If we see that we need to generalize better, we might look into reducing the number of weights and parameters in the model. It depends on the results.

Once we have our best-tuned model as a result of our experiments according to the development accuracy, we plan on finally running this model on the test set. That way, we get the most unbiased metric for the performance of our model and something to compare to the current state of the art.

Additionally, we will do some analysis about the state of the art and our model, analyzing the resources, amount of tuning, and other constraints of our research compared to the current state of the art and explore given those factors the success level of our project.
