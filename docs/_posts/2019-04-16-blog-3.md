---
layout: post
title:  "Blog 3: Project Proposal"
---

NLVR2 is a dataset that focuses on VQA with image pairs and a caption. The task is to determine whether or not the caption is true or false based on the image pair. For example, you might have two images of bottles where the caption is “there are 8 bottles in total”, and you would need to determine if this is true or false.

## Motivation

The topic of reasoning with joint visual and linguistic has gained more and more interest from the research community. Naturally, many datasets, and models trained on those datasets, have been developed for this task.  One particular subset of tasks, commonly known as Visual QA, have as inputs one or more images and a question in the natural language, from which a model is supposed to derive an answer.

NLVR, standing for Natural Language Visual Reasoning, is a dataset for one of those tasks where given a pair of images and a caption, the model determines whether or not the caption is true or false based on the pair. Unfortunately, these datasets often do not reflect the full complexity of the problem.  NLVR, for instance, has synthetically generated images as its images instead of “natural” photographs that occur in the real world. In response, NLVR2 was made that used natural photos from the web. As to be expected, our current models do not have good performance on this task, with the best test accuracy on the leaderboard currently being roughly 54%, barely higher than the random 50%.

Thus, there is clearly a lot of work and chances for improvement at this task. The shift from synthetic to real images requires the language processing of the caption to be more complex and more thought out to be able to capture the reasoning that intersects between these visual and linguistic elements. Improving performance on this problem will help forward the field of reasoning in natural language processing, especially in regards with visual inputs, and has wide implications on joint visual and linguistic understanding for our models.

## Objectives

This project presents an opportunity to learn and improve upon the domain of reasoning at the intersection between vision and language. Given that current state-of-the-art models on this task have lackluster performance, improving the performance on this task and thus furthering research on Visual QA, especially with the constraints of resources for this capstone, is a goal of this project. This project is not just limited to finding a good model though.  It is our hope that we can discover certain modules or architectures that are well-suited towards Visual QA and that we understand the possible intuitions for why that might be, allowing other researchers to take inspiration from these modules or architectures for their own tasks.

## Proposed Methodologies

NLVR^2 has several aspects that make the dataset hard to crack. Consequently, we need to find solutions to overcome these challenges in order to get any viable results on these datasets. We describe these aspects and following methodologies here. First, the dataset is extremely linguistically complex. Phrases such as “the woman has a scarf that hangs below her shoulder on the right side” is extremely hard to understand - on top of this, the dataset includes the same phrase in several different image pairs so it’s hard to “cheat” like in VQA. To overcome this, we propose using ELMo/BERT or other methods to understand the sentence from a conceptual standpoint. On the image side of things, the object detection is extremely complex - from the differences between “glass table” to “wooden table”. To overcome this, we can use our understanding from linguistics (for example, dense captioning). In addition, we propose combining linguistics and image processing - for example, several models examine the caption and the image one after another (like a neural network) and we can do a similar method to gain a greater understanding of the image pairs.

There are also a couple other methodologies that we can use. One pulls from the neural module networks paper - here, we parse the caption using a dependency tree, then use that semantic understanding to create a method in which we can examine the image in question. For example, for a caption such as “the tree is brown and black”, we would parse the caption into “is(and(brown, black))” and use this to examine the image and determine a truth value. We would likely need to adjust this method for NLVR2 as it is much more linguistically complex than NLVR2 or any other dataset that it has been tested on in the past.

We can also attempt a simple method where we preprocess the image via a CNN and use that vector as a basis level of image understanding. For example, it is likely that there is a pattern in all vectors that mention “brown dog”, so we can use that pattern along with the image captions to train truth values for the image pairs.

## Minimum Viable Action Plan

First, we would build the baseline implementations mentioned in the original paper, which are the RNN+CNN and MaxEnt models. Second, we would improve these baselines by planning and testing the specific test failure cases of the RNN+CNN and MaxEnt models and changing the model to improve performance on the domains of test examples that had a lot of failed cases, adjusting parameters, modules, or even architecture changes. Third, we would look at the failure and success cases of our new model and implement further improvements.

## Evaluation Plan

To evaluate our model during the development of this project, we will mainly be using the development set. Ideally, we would only evaluate on the test set once after all the potential models we would have developed have been finalized. These evaluations will mainly be using model accuracy, which is the percentage of task instances in these sets where the model being evaluated correctly determines whether or not the caption is true or false regarding the image pair.

This project’s success will mainly revolve around how well we fulfill our objectives. This would be holistic on certain criteria of success. Having comparable performances to the basic baselines would indicate a level of success. Having models that show improvement in certain domains of problems over the state-of-the-art would indicate a level of success. Of course, if we manage to beat the state-of-the-art in general, or even possibly reaching an accuracy of 60%, would be an outstanding success.

## Stretch Goals

Beat the test accuracy of the MaxEnt model.
Improve our test accuracy to a more plausible accuracy number (~60%).
Design and test a new model architecture, possibly involving modules.
Try different CNN implementations (like skips).

## Available Resources

http://lic.nlp.cornell.edu/nlvr/ : Original resource containing the dataset and leaderboard of state of the art

https://allennlp.org/: Resource for NLP state-of-the-art

https://keras.io/: Resources for image processing

https://pytorch.org/: Resource for general ML framework / image processing that works well with AllenNLP

https://ai.google.com/research/ConceptualCaptions/: Captions dataset that we could possibly train on

https://github.com/jcjohnson/densecap: Captioning method that we could use to help generate data to train our model on.

## Related Work

https://arxiv.org/pdf/1709.07871.pdf : FiLM: Visual Reasoning with a General Conditioning Layer
This is a paper detailing the methods used by MILA in collaboration with Cornell University, which achieved a 52.1% accuracy on the public test set.
The idea proposed by the paper was to introduce a feature-wise affine transformation on the intermediate features of the neural network.

https://arxiv.org/pdf/1811.00491.pdf : A Corpus for Reasoning About Natural Language Grounded in Photographs
This is a paper from Cornell that provides general baselines for this task, as well as the current state of the art for this task.
Their MaxEnt model achieves the best performance of 54.8%, CNN+RNN gets 52.4%, CNN only achieves 51.9%, RNN only achieves 51.1%.
This also specifies that the majority class achieves a similar accuracy of 51.1%.
For our initial starting point, we are most likely going to do something similar in the form of a CNN+RNN using embeddings (which this paper also does)

https://arxiv.org/pdf/1704.05526.pdf : Learning to Reason: End-to-End Module Networks for Visual Question Answering
This is a paper from Berkeley that achieved 51.1% on the public test set using an End-to-End Module Network.
Their idea was to build on standard neural module networks, which generate network structures for each subcomponent extracted from the query, by creating a End-to-End Module Network. Essentially, it is treating the QA problem as a series of subtasks.
The N2NMN's uses a set of modules to create an attention map over the images (for example a find module to locate corresponding objects from the text)
