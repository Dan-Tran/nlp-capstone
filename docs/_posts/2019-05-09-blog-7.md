---
layout: post
title:  "Blog 7: Continuing First Advanced Solution Attempt"
---

## What We Did

As mentioned in our previous blog post, we worked on having a better utilization of our parsing information in terms of incorporating that information into our model.  We felt that simply concatenating the encodings of this information to the encoding of the text is inadequate as the test is “encoded” without this information we have. Therefore, we instituted a few changes in continuing with our advanced solution attempt.

Here, we still run the sentences, tags, heads, and dependencies through their own embedders, with the exception of the heads of course which uses the same embedders as the sentences.  However, these embeddings are then concatenated with each other, with each “word” embedding in the sequences being concatenated with its associated tag, head, and dependency embedding. This concatenated embedding is then feed into a single RNN encoder, a bidirectional LSTM to be specific.

The specification for this model are as follows. The CNN portion of the model remains the same as in the previous blog post. The RNN portion has the sentence first run through the universal dependency model to get the predicted tags, heads, and dependencies. All four are then embedded. The sentences are embedded and heads are embedded with the same 100 dimensional GloVe embeddings as in the previous blog post. The tags are embedded with a basic 50 dimensional embedder and the dependencies are embedded with a different basic 50 dimensional embedder. These embeddings are then concatenated together, with each word vector concatenated with its associated tag, head, and dependency vector. This concatenated embeddings is then fed into a bidirectional LSTM with a hidden size of 500 and 2 layers. This encoding is then concatenated with the two image encodings and feed into a feedforward network as with the previous blog post.

## Experiments

One of the feedback we received from the previous blog post was to increase the number of epochs in our training iteration to allow the model to learn how to use the information more effectively.  Accordingly we increased the training time and the number of epochs. However, with this increase we had inconclusive results, with no significant improvements in our evaluation. There was also a lot of GPU hiccups and the fact that each epoch took around 4 hours that hindered us from doing the in depth testing we would like.

Additionally, with the new incorporation of parsing information in our model, we trained that new and hopefully improved model with our training set. With this, we were able to achieve a 53% training accuracy and a 52% development accuracy. This is super promising as it shows an improvement to our baseline models that we have made in the previous blog post and shows that we are in the right track. Unfortunately, due to various technical circumstances, we were unable to train it for more the 5 epochs, but this shows we are progessing with our project.

## Error Analysis

In doing our error analysis, from our observations there were no significant distinctive trends amongst the data points we ingested. Between the different categories of questions, like counting, comparisons, and the like, there was no significant difference in the evaluation of our model on these separate categories of training instances.  The task itself was also trivial when we attempted to do the labeling ourselves, which suggests that there is a valid connection/relation between the input and label. Given that our models are performing about the same as random guessing and majority class, it makes sense that trends were not seen. This points to two ideas. The first is that the visual input is not being properly extracted. The second is that the amount for semantic reasoning from the text is insufficient. Given that this is an NLP course, we are planning to look more into how to distill information from the prompts, with a less significant focus on the visual input.

## Next Steps

Moving on from our first solution attempt, we hope to incorporate more information through various modules that we can use in our model. For example, we are looking into whether or not adding the outputs from object detection on the images could improve our performance during evaluation. Similarly, more advanced reasoning modules could also improve performance on the text side. We will look into which of these approaches we would like to take and hopefully realize them with our second solution attempt.

## Group Discussion

As a group, we feel that we are making steady, but continuous progress, with no major axis that needs direct attention. So far we have hit all the milestones with minimal panic and our models have been training (albeit with varying efficacy and speed). One thing that could be improved is our communication and coordination between members. Currently, the main modes of communication are Messenger when not in class and in person during scheduled course hours. To address this, we want to meet more often outside of the course to work on the project as a group. While progress is slowly being made, we work faster/make more progress when working simultaneously in the same room. 

On top of these realizations, we also need to brainstorm more about how we approach the problem. As mentioned before we are meeting the milestones, but they should be minimum, not a motivator. 
