---
layout: post
title:  "Blog 6: Initial Advanced Solution Attempt"
---

## What We Tried

This week we attempted our first advanced solution, where we introduced Universal Dependencies into our model. Here, we are trying to understand our data from a semantic perspective. Expanding on the RNN+CNN model that we introduced in Blog 5, we inserted, for each sentence:

* the part-of-speech tags for each of the tokens
* the "head" for each of tokens 
* the predicted dependency ("punct", "conj", ...)

Each of these attributes was first tokenized, then embedded, then appended to the embedded versions of the two images and original text. This is then passed through a feedforward.

For this version of the model, the images were passed through 4 convolution layers (going from 3 -> 8 -> 16 -> 32 ->64 channels) and after each convolution layer the output was passed to a max-pooling layer. The last two convolution layer used relu activation.

The text itself is passed through a 2 layered bi-directional lstm with 100d glove embeddings (Similar to the previous blogs).

Each of the introduced Universal Dependency features is passed through a basic 10-dimensional token embedder and a 2 layered bi-directional lstm encoder with a hidden size of 20.

All other above components were then flattened, concatenated, and passed through a feedforward layer to get the final prediction.

Due to limited resources, this model was trained for 5 epochs.

## Exciting Results

When implementing our model, we were extremely worried about the feasibility of training and developing the model due to having multiple interacting components and sub-models. However, it was exciting for us to see that the code for the model did run and train. Granted, each epoch took around 3-4 hours to execute.

## Confusing Results

Unfortunately, after running several epochs, our results were still close to the results for the baseline results. This was confusing because we thought that adding a parse representation to our model would help the model gain more understanding of the input. However, it doesn’t seem to be able to train beyond the results that we got in the baselines. Some explanations for this could be the relatively simple way this extra data was incorporated into our model. The encoding of the tags, heads, and dependencies were simply concatenated to the original caption encoding and image encodings before being fed into the feedforward network. Better utilization of this additional information in our model could improve performance more effectively and needs to be looked into.

## Failure Modes

One of the biggest reasons why our data can fail is that the semantic understanding isn’t being used to understand the image. For example, even if we add a full semantic understanding of our text, our model could still only match the text-only baseline (because of the way the data is set up). This is a problem because it means that the text-only improvements will get nowhere without enabling some kind of image recognition, and so, it is really hard to understand the data through only the text. To get over this failure mode, we can enable some kind of image recognition defined by a semantic understanding of the text.

Another big reason why our data would fail is that one sentence can have several different parses that are correct. Therefore, we might have two very similar sentences that have vastly different parses. This is especially a bigger problem because as the sentences get more complex, the sentence representations can get more and more complicated. We can resolve this failure mode by determining some kind of correctness or comparison beyond a feedforward network for our results from our parse.

## Next Steps

Moving forward, we would like to perform some error analysis on the results of our new advanced model to understand the current failings of our approach. This will be important so that we can better develop approaches to improve our model in evaluation. In addition, we would like to optimize our model due to the extreme amount of time it has been taking to run an epoch in training. Being able to pretrain certain aspects of our model and save those for future use so that we don’t have to run those aspects again would significantly make our training more efficient. Currently, our model is running our tags, heads, and dependencies through an embedding and encoding module for each, then concatenating the output with the image and text encoding as mentioned in Blog 5. In the future, we would like to have better methods of incorporating the information from Universal Dependencies to our models other than just concatenating it, like using it in conjunction with the text for encoding. Using this information to better understand the image as well is something we would like to look into in the future.
