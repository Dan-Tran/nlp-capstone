---
layout: post
title:  "Blog 8: Second Advanced Solution Attempt"
---

## What We Tried

This week, we attempted our second advanced solution, where we introduced the detection of objects in our model. Here, we are trying to learn more information about our pair of images by detecting the objects in those images. Expanding on the first advanced solution attempt in Blog 7, we inserted for each training instance a list of objects for each image, with each element in this list having the type of object, the probability, and a tuple representing its bounding box.

The type of object, being a word represented as a string, are tokenized and embedded. This embedding vector is then concatenated with the probability and bounding box coordinates into one vector. Thus, we have multiple concatenated vectors for the multiple objects detected for each image.  This sequence of vectors are then encoded to single vector with a 2 layered bi-directional LSTM with a hidden size of 100. This encoding for each image is the concatenated with the original image encoding and text encoding before being passed into the feedfoward network as in Blog 7.

For this version of the model, instead of using the CNN in our model as in Blog 7, we preprocessed all of the images with ResNet, saving those resulting vectors and using those as the image encoding that the CNN portion would have produced. This marked a great improvement in terms of speed and significantly reduced training time.

The text and Universal Dependency features were passed through the same embeddings and encodings as in the previous blog.

The type of object was embedded with the same 100 dimensional GloVe vector as the caption and the associated LSTM is as mentioned above.

All components were then flattened, concatenated, and passed through a feedforward layer to get the final prediction.

Due to limited resources, this model was trained for 5 epochs.

The code for this attempt is located at [https://github.com/Dan-Tran/nlp-capstone/tree/master/blogcode/blog8](https://github.com/Dan-Tran/nlp-capstone/tree/master/blogcode/blog8), with `testmodel.py`, `plaintext_reader.py`, and `test.jsonnet` being the code used for this attempt.

## Exciting Results

Given that our previous models were extremely slow to train, we set out to pre-empt this by pre-training our object detector. 
We first tried to do this with Joseph Redmon's YOLOv3 but found soon after that this would take days to finish preparing (the fastest public implementation was hardware dependent, so we used a slower one). Since this was not feasible, we decided to switch to YOLOv3-tiny which has a worse MaP than the original YOLOv3 model but trains much faster (within a day). It was exciting for there to have semi-tangible results.
We show that precomputing the images results in a ~80% reduction in train time (4 hours > 20 mins). So far in our extremely limited training, we were able to have comparable results of around 51.5% to our previous model in Blog 7 and with more epochs we are expecting higher performance.

## Confusing Results
Pretraining a large amount of images means a massive GPU workload, so as mentioned we were forced to use the tiny version due to time constraints, with the full version still processing. In addition, our epochs still take 20 minutes each, which is slightly troublesome because it is hard to train when the epochs still take so long. We plan to use the full object detection model in the future (as it finishes processing tonight), which should hopefully help with accuracy numbers.

## Failure Modes

One of the reasons why the object detection is not as impactful is that the values associated with the bounding boxes do not have very much context in respects to the rest of the embedded/encoded attributes in the model. This could be a problem because the information extracted may be underutilized/undertrained.

Another reason for why our data would fail is that the object detector we are using is far weaker and intensive than the main implementation of YOLOv3. While unlikely, inaccurate classification and bounding boxes could negatively influence how the query text interacts with the image data. 

## Next Steps

For potential improvements of our model, we could look into using the full YOLOv3 or other object detection models and exploring ways to efficiently use that model for our image detection tasks. This would probably involve finding the resources, hardware, and software to make sure that the pre-training does not take weeks. We should also analyze where the model fails even with the usage of object detection information. That way, we can see generally what tasks we are failing at and make further modifications to our model in incorporating the object detection features. In addition, currently building the vocabulary takes around 4 hours due to having to parse each caption every time we want to train the model. Therefore, preprocessing that would be a next step we would like to pursue. Furthermore, we can use the object detection info more effectively and in more ways.  One avenue could be having a feature if the caption contains the word for a detected object in the left image for example. We are looking into adding as many of these extracted features as feasible.
