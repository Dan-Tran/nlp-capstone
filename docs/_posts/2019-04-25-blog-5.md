---
layout: post
title:  "Blog 5: RNN + CNN Baseline"
---

Here, we completed the second baseline mentioned in [Suhr et al. 2018](https://arxiv.org/pdf/1811.00491.pdf). This baseline concats the RNN used in the previous baseline along with each of the images passed through CNN. This results in an approximately 16,000 dimensional vector that is then passed through an multilayer perceptron and makes a prediction. This uses the text and image together, and is aptly described the "text + image" baseline in the NLVR2 paper, also known as RNN+CNN.

We used AllenNLP and convolution modules from PyTorch to implement the model and run our experiments on the given data. We ran each image of the training instance through two Conv2D layers and then a MaxPool layer. It continued to run the results of the MaxPool layer through two Conv2D layers and then through two average pooling layers. It then takes the final result of this CNN and flattens it to produce a vector for each image. Finally, it takes those vectors and concatenates it to the output of the RNN presented in the last section and feeds it through a feedforward layer to finalize a prediction.

With this model, we were able to achieve a 98.7% accuracy on the training data and a 51.4% accuracy on the development data. Our presented evaluation framework is the dev accuracy, which is given as 51.4%. This is an improvement on the text-only baseline presented in [Suhr et al. 2018](https://arxiv.org/pdf/1811.00491.pdf). This is slightly lower than the RNN+CNN development accuracy presented in the paper, but it is expected.  This is due to the limited amount of resources we have for our project compared to the paper authors and we only trained our RNN+CNN baseline for 15 epochs.

We note here that there is a huge disparity between the training accuracy and the development accuracy, with the training accuracy being roughly 45 percentage points above the development accuracy.  This is a clear instance of severe overfitting in our model and is something we will have to take into count as we work toward our more advanced solutions.

The code for this baseline is located at [https://github.com/Dan-Tran/nlp-capstone/tree/master/blogcode/blog5](https://github.com/Dan-Tran/nlp-capstone/tree/master/blogcode/blog5).
