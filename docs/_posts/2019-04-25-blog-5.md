---
layout: post
title:  "Blog 5: RNN + CNN Baseline"
---

Here, we completed the second baseline as mentioned in [Suhr et al. 2018](https://arxiv.org/pdf/1811.00491.pdf). This baseline concatenates the RNN output used in the previous baseline along with each of the images passed through a CNN. This results in an approximately 16,000 dimensional vector that is then passed through an multilayer perceptron and makes a prediction. This uses the text and image together, and is aptly described the "text + image" baseline in the NLVR2 paper, also known as RNN+CNN.

We used AllenNLP and the convolution modules from PyTorch to implement the model and run our experiments on the given data. For each image of the training instance, we feed those images to the CNN model as follows. The image is first fed into two convolutional layers, each outputting 64 channels with a stride of 3. This was then fed into a max pooling layer with a stride of 2. After that, the result is fed into two more convolutional layers, each outputting 128 channels with a stride of 3 with ReLU activation. This was then fed into an average pooloing layer with a stride of 2. The resulting output is flattened into a vector for the image, and this is done for each image in the training instance of which there are two. Finally, it takes those vectors and concatenates it to the output of the RNN portion presented in the last blog post and feeds it through an 11 layer feedforward layer to finalize a prediction.

With this model, we were able to achieve a 98.7% accuracy on the training data and a 51.4% accuracy on the development data. Our presented evaluation framework is the dev accuracy, which is given as 51.4%. This is an improvement on the text-only baseline presented in [Suhr et al. 2018](https://arxiv.org/pdf/1811.00491.pdf). However, this is slightly lower than the RNN+CNN development accuracy presented in the paper, but that is to be expected.  This is due to the limited amount of resources we have for our project compared to the paper authors and the fact that we only trained our RNN+CNN baseline for 15 epochs.

We note here that there is a huge disparity between the training accuracy and the development accuracy, with the training accuracy being roughly 45 percentage points above the development accuracy.  This is a clear instance of severe overfitting in our model and is something we will have to take into count as we work toward our more advanced solutions.

The code for this baseline is located at [https://github.com/Dan-Tran/nlp-capstone/tree/master/blogcode/blog5](https://github.com/Dan-Tran/nlp-capstone/tree/master/blogcode/blog5).
