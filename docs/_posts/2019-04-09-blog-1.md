---
layout: post
title:  "Blog 1: Team Information"
---
## Team Name

DJ<sup>2</sup>

## Team Members

Dan Tran, Jack Khuu, and Jeff Da.

## Project Ideas

### NLVR2 - Visual QA with Real Images.

NLVR2 is a dataset that focuses on VQA with image pairs and a caption. The task is to determine whether or not the caption is true or false based on the image pair. For example, you might have two images of bottles where the caption is "there are 8 bottles in total", and you would need to determine if this is true or false.

#### Minimum Viable Action Plan

First, we would build the baseline implementations mentioned in the original paper, which are the RNN+CNN and MaxEnt models. Second, we would improve these baselines by planning and testing the specific test failure cases of the RNN+CNN and MaxEnt models and changing the model to improve performance on the domains of test examples that had a lot of failed cases, adjusting parameters, modules, or even architecture changes. Third, we would look at the failure and success cases of our new model and implement further improvements.

#### Stretch Goals

* Beat the test accuracy of the MaxEnt model.
* Improve our test accuracy to a more plausible accuracy number (~60%).
* Design and test a new model architecture, possibly involving modules.
* Try different CNN implementations (like skips).

### VCR - Visual Commonsense Reasoning.

VCR is a dataset that focuses on cognition-level visual reasoning in the natural language. The task is to answer challenging visual questions experessed in the natural language and also provide rationale explaining why the answer is true. More specifically, the model must pick one of four multiple choice answers based off of a given image and provide reasoning in the natural language, often involving information not implicit in the text.

#### Minimum Viable Action Plan

First, we would build the text-only baselines as mentioned in the original paper. Then, we would add some implementation of a VQA infrastructure (e.g. concatenate a ResNet model on top of the text-only baselines). Then, we would plan and develop a new model based off the current state of the art model for this task, like R2C. Then, we could fine-tune this model if it works, or figure out why it doesn't work and fix the model so that it does.

#### Stretch Goals

* Beat the test accuracy of the text-only baselines.
* Beat the test accuracy of the state of the art.
* Create a new model infrastructure based on past research.

### ReCoRD - Commonsense reading comprehension.

Record is a dataset that focuses on machine reading comprehension requiring commonsense reasoning. The task is, for a given passage and a set of text spans from the passage and a cloze-style query with a missing text span, to choose a text span from the set of text span to complete the cloze-style query that becomes a detail supported by the passage.

#### Minimum Viable Action Plan

First, we would reimplement the baselines presented in the original paper, like the DotQA model using ELMo or BERT. Next, we would select the test categories where the model has a high rate of failures and determine the similarities between those. Based on that, we would then build and create a model that attempts to improve the performance on those categories where the current state of the art fails.

#### Stretch Goals

* Beat or hit current test accuracy baselines with DocQA and ELMo.
* Create significant analysis on results.
* Create a new model and infrastructure that improves the baseline test accuracy.

## Github Project

This project will be hosted at [https://github.com/Dan-Tran/nlp-capstone](https://github.com/Dan-Tran/nlp-capstone).
